{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-indiana",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "clinical-rating",
   "metadata": {},
   "source": [
    "### CCBM 2023 Summer Programming Workshop\n",
    "# Day 8 - Segment 2\n",
    "\n",
    "### Machine Learning Mini-Challenge\n",
    "Use the data in the file \"titanic_dataset.csv\" (adapted from Kaggle) to predict survival on the Titanic using logistic regression and the data preparation methods described above. Evaluate the model by analyzing its accuracy.\n",
    "\n",
    "The variables in the dataset consist of:\n",
    "- PassengerID - Identification number\n",
    "- Survived - Survival (1 = Yes, 0 = No)\n",
    "- PClass - Passenger Class (1 = 1st class, 2 = 2nd class, 3 = 3rd class)\n",
    "- Name - Name\n",
    "- Sex - Gender\n",
    "- Age - Age\n",
    "- SiblingSpouse - Number of siblings and spouses on board with the passenger\n",
    "- ParentChild - Number of parents and children on board with the passenger\n",
    "- Ticket - Ticket number\n",
    "- Fare - Fare\n",
    "- Cabin - Cabin\n",
    "- Embarked - Geographic location from which passenger depart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import libraries to score our predictive models\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Import libraries to On Hot Encode our categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 5, 4\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-craft",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic_dataset.csv\")\n",
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the dataframe for missing values and data types\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the target variable is binary\n",
    "titanic[\"Survived\"].unique()\n",
    "\n",
    "# Is survivability a balanced class?\n",
    "titanic[\"Survived\"].value_counts()\n",
    "\n",
    "# Use seaborn's countplot to observe this\n",
    "sns.countplot(x=\"Survived\", data=titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns. \n",
    "# Assume the PassengerID, Name and Ticket are not relevant to survival\n",
    "df = titanic.drop(columns=['PassengerId', 'Name', 'Ticket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the dataset and check for missing values\n",
    "df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Cabin\" column contains too many null values. Drop it.\n",
    "df.drop(columns=\"Cabin\", inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in the \"Age\" variable\n",
    "df.hist(column=\"Age\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ParentChild variable to guide imputing Age\n",
    "df.groupby(by=df.ParentChild).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we impute according to the ParentChild AND Sex variables?\n",
    "display(df.groupby(by=[df.ParentChild, df.Sex]).mean())\n",
    "\n",
    "df[[\"ParentChild\", \"Sex\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-heading",
   "metadata": {},
   "source": [
    "The number of passengers traveling with three or more parents and children make up less than 2% of the total number of passengers in this dataset. Let's keep it simple and impute according the mean Age of the passenger and their parent-child companions and ignore gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to impute values for Age according to presence of null and \n",
    "# the ParentChild value\n",
    "\n",
    "def approx_age(lst):\n",
    "    Age = lst[0]\n",
    "    ParentChild = lst[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        if ParentChild == 0:\n",
    "            return 32\n",
    "        elif ParentChild == 1:\n",
    "            return 24\n",
    "        elif ParentChild == 2:\n",
    "            return 17\n",
    "        elif ParentChild == 3:\n",
    "            return 33\n",
    "        elif ParentChild == 4:\n",
    "            return 45\n",
    "        elif ParentChild == 5:\n",
    "            return 39\n",
    "        else:\n",
    "            return 43  \n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .apply() to apply the function, approx_age to the Age column\n",
    "df['Age'] = df[['Age', 'ParentChild']].apply(approx_age, axis=1)\n",
    "\n",
    "# Check for the existence of null values in the dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining two null values in the Embarked column\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "display(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to more accurately reflect the cleaned dataset\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(df.index)\n",
    "list(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-strengthening",
   "metadata": {},
   "source": [
    "Now that the null values have been removed from the dataset, we need to encode the remaining categorical variables to dummy variables (1,0). The only two remaining categorical variables are:\n",
    "- Sex\n",
    "- Embarked\n",
    "\n",
    "We can use pandas .replace() method to encode \"Sex\" with bummy variables (1,0).\n",
    "We'll need to one hot encode the \"Embarked\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the \"Sex\" and \"Embarked\" variables are categorical and assess the number of\n",
    "# categories for each\n",
    "df.Sex.unique(), df.Embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert male/female to 1/0 in the \"Sex\" feature\n",
    "df[\"Sex\"].replace(to_replace=[\"male\", \"female\"], value=[1,0], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to rename the \"Sex\" column to \"male\" since \"1\" means the passenger is male and\n",
    "# \"0\" means the passenger is female\n",
    "df.rename(columns = {\"Sex\": \"male\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the \"Embarked\" variable, we can encode the categories, in alphabetical order, as \n",
    "# \"Cherbourg\" = 0, \"Queenstown\" = 1, and \"Southampton\" = 2 and then we'll one hot encode this\n",
    "# dataframe since it is a multi-nomial variable\n",
    "embarked = df.Embarked\n",
    "embarked.replace({\"Cherbourg\": 0, \"Queenstown\": 1, \"Southampton\":2}, inplace=True)\n",
    "\n",
    "# One Hot Encode the embarked dataframe\n",
    "embarked_encoder = OneHotEncoder(categories=\"auto\")\n",
    "embarked_ohe = embarked_encoder.fit_transform(embarked.to_numpy().reshape(-1,1))\n",
    "embarked_ohe\n",
    "\n",
    "embarked_mx = embarked_ohe.toarray()\n",
    "embarked_mx[:10]\n",
    "\n",
    "# Convert the array into a pandas dataframe\n",
    "embarked_df = pd.DataFrame(embarked_mx, columns = [\"Cherbourg\", \"Queenstown\", \"Southampton\"])\n",
    "embarked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Embarked\" column in the original titanic dataframe, df\n",
    "# and concatenate embarked_df into the original\n",
    "\n",
    "df.drop(columns=\"Embarked\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.concat([df, embarked_df], axis=1)\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for independence between the explanatory variables\n",
    "titanic_df.iloc[1:, 1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for independence between the explanatory variables\n",
    "sns.heatmap((titanic_df.iloc[1:, 1:].corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Cherbourg feature since there is collinearity between it and Southampton\n",
    "titanic_df.drop(columns=\"Cherbourg\", inplace=True)\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy and evaluate the model. First, split the data into train and test sets\n",
    "# Use default test siize, 75%:25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    titanic_df.iloc[:, 1:],\n",
    "    titanic_df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression model\n",
    "lr = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Fit the model to our data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make a prediction on the X_test data\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# and evaluate model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-yeast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
